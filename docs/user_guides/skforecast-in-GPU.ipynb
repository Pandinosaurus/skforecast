{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5148957",
   "metadata": {},
   "source": [
    "# Skforecast in GPU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ce2a3d8",
   "metadata": {},
   "source": [
    "Traditionally, machine learning algorithms have been executed on CPUs (Central Processing Units)â€”general-purpose processors capable of handling a wide variety of tasks. However, CPUs are not optimized for the highly parallelized matrix operations that many machine learning algorithms rely on, often resulting in slower training times and limited scalability. In contrast, GPUs (Graphics Processing Units) are specifically designed for parallel processing, capable of performing thousands of simultaneous mathematical operations. This makes them particularly well-suited for training and deploying large-scale machine learning models.\n",
    "\n",
    "Several popular machine learning libraries have implemented GPU acceleration, including **XGBoost**, **LightGBM**, **CatBoost** and **CuML**. By leveraging GPU capabilities, these libraries can dramatically reduce training times and enhance scalability. The following sections demonstrate how execute **skforecast** with GPU acceleration to build efficient and powerful forecasting models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b09bde5",
   "metadata": {},
   "source": [
    "<div class=\"admonition note\" name=\"html-admonition\" style=\"background: rgba(0,184,212,.1); padding-top: 0px; padding-bottom: 6px; border-radius: 8px; border-left: 8px solid #00b8d4; border-color: #00b8d4; padding-left: 10px; padding-right: 10px;\">\n",
    "\n",
    "<p class=\"title\">\n",
    "    <i style=\"font-size: 18px; color:#00b8d4;\"></i>\n",
    "    <b style=\"color: #00b8d4;\">&#9998 Note</b>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "\n",
    "The performance advantage of using a GPU depends heavily on the specific task and the size of the dataset. Generally, GPU acceleration offers the greatest benefits when working with large datasets and complex models, where its parallel processing capabilities can significantly reduce training times.\n",
    "\n",
    "In recursive forecasting (<code>ForecasterRecursive</code> and <code>ForecasterRecursiveMultiseries</code>) the <b>prediction phase must be executed sequentially since each time step depends on the previous prediction</b>. This inherent dependency prevents parallelization during inference, which explains why model fitting is substantially faster on a GPU, while prediction can actually be slower compared to using a CPU. To overcome this limitation, <b>skforecast automatically switches the regressor to use the CPU for prediction</b>, even if it was trained on a GPU.\n",
    "\n",
    "In contrast, direct forecasters (<code>ForecasterDirect</code>, <code>ForecasterDirectMultivariate</code>) do not rely on previous predictions during inference. This lack of dependency allows both training and prediction to fully benefit from GPU acceleration.\n",
    "\n",
    "</p>\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b9fd63a",
   "metadata": {},
   "source": [
    "<div class=\"admonition note\" name=\"html-admonition\" style=\"background: rgba(0,184,212,.1); padding-top: 0px; padding-bottom: 6px; border-radius: 8px; border-left: 8px solid #00b8d4; border-color: #00b8d4; padding-left: 10px; padding-right: 10px;\">\n",
    "\n",
    "<p class=\"title\">\n",
    "    <i style=\"font-size: 18px; color:#00b8d4;\"></i>\n",
    "    <b style=\"color: #00b8d4;\">&#9998 Note</b>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "\n",
    "Despite the significant advantages offered by GPUs (specifically Nvidia GPUs) in accelerating machine learning computations, access to them is often limited due to high costs or other practical constraints. Fortunatelly, <b>Google Colaboratory (Colab)</b>, a free Jupyter notebook environment, allows users to run Python code in the cloud, with access to powerful hardware resources such as GPUs. This makes it an excellent platform for experimenting with machine learning models, especially those that require intensive computations. The following links provide access to Google Colab notebooks that demonstrate how to use skforecast with GPU acceleration.\n",
    "</p>\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"https://colab.research.google.com/drive/10PYQFQN9oNkAHh0X7wwyBLQ3JQ_Cm7pP?usp=sharing\">Skforecast in GPU: XGBoost</a></li>\n",
    "    <li><a href=\"https://colab.research.google.com/drive/17Csc70AY-GQA-tvZjq9TYCbmnrNOzslh?usp=sharing\">Skforecast in GPU: LightGBM</a></li>\n",
    "    <li><a href=\"https://colab.research.google.com/drive/1Z-n0kKEnQvY02e9-HxKbkTdLc10RNd_-?usp=sharing\">Skforecast in GPU: CatBoost</a></li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ece7fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skforecast version : 0.16.0\n",
      "xgboost version    : 2.1.2\n",
      "lightgbm version   : 4.5.0\n",
      "catboost version   : 1.2.8\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import psutil\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm\n",
    "from lightgbm import LGBMRegressor\n",
    "import catboost\n",
    "from catboost import CatBoostRegressor\n",
    "import warnings\n",
    "\n",
    "import skforecast\n",
    "from skforecast.recursive import ForecasterRecursive\n",
    "from skforecast.model_selection import backtesting_forecaster, TimeSeriesFold\n",
    "\n",
    "print(f\"skforecast version : {skforecast.__version__}\")\n",
    "print(f\"xgboost version    : {xgboost.__version__}\")\n",
    "print(f\"lightgbm version   : {lightgbm.__version__}\")\n",
    "print(f\"catboost version   : {catboost.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7efc087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "NVIDIA T1200 Laptop GPU\n",
      "Memory Usage:\n",
      "Allocated : 0.0 GB\n",
      "Reserved  : 0.0 GB\n",
      "CPU RAM Free: 10.91 GB\n"
     ]
    }
   ],
   "source": [
    "# Print information abput the GPU and CPU\n",
    "# ==============================================================================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated :', round(torch.cuda.memory_allocated(0) / 1024**3, 1), 'GB')\n",
    "    print('Reserved  :', round(torch.cuda.memory_reserved(0) / 1024**3, 1), 'GB')\n",
    "\n",
    "print(f\"CPU RAM Free: {psutil.virtual_memory().available / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2db9943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1990-01-01 00:00:00    1.20438\n",
       "1990-01-01 01:00:00    0.21551\n",
       "Freq: h, Name: y, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "# ==============================================================================\n",
    "n = 1_000_000\n",
    "data = pd.Series(\n",
    "    data  = np.random.normal(size=n), \n",
    "    index = pd.date_range(start=\"1990-01-01\", periods=n, freq=\"h\"),\n",
    "    name  = 'y'\n",
    ")\n",
    "data.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "074ca42f",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05bc5e0",
   "metadata": {},
   "source": [
    "To run an XGBoost model (version 2.0 or higher) on a GPU, set the argument device='cuda' during initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "516970c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "# ==============================================================================\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\".*Falling back to prediction using DMatrix.*\",\n",
    "    category=UserWarning,\n",
    "    module=\"xgboost\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "895b3ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time using GPU: 0 days 00:00:28.888429\n",
      "Prediction time using GPU: 0 days 00:00:00.091795\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db37da711204252b56157d812cd22ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backtesting time using GPU: 0 days 00:00:50.713341\n"
     ]
    }
   ],
   "source": [
    "# Create and train forecaster with a XGBRegressor using GPU\n",
    "# ==============================================================================\n",
    "forecaster = ForecasterRecursive(\n",
    "                 regressor = XGBRegressor(\n",
    "                                 n_estimators = 1000,\n",
    "                                 device       = 'cuda',\n",
    "                                 verbosity    = 1\n",
    "                             ),\n",
    "                 lags = 50\n",
    "             )\n",
    "\n",
    "start_time = pd.Timestamp.now()\n",
    "forecaster.fit(y=data)\n",
    "elapsed_time = pd.Timestamp.now() - start_time\n",
    "\n",
    "print(f\"Training time using GPU: {elapsed_time}\")\n",
    "\n",
    "# Predict\n",
    "# ==============================================================================\n",
    "start_time = pd.Timestamp.now()\n",
    "forecaster.predict(steps=100)\n",
    "elapsed_time = pd.Timestamp.now() - start_time\n",
    "\n",
    "print(f\"Prediction time using GPU: {elapsed_time}\")\n",
    "\n",
    "# Backtesting\n",
    "# ==============================================================================\n",
    "cv = TimeSeriesFold(\n",
    "         steps              = 100,\n",
    "         initial_train_size = 990_000,\n",
    "         refit              = False,\n",
    "         verbose            = False\n",
    "     )\n",
    "start_time = pd.Timestamp.now()\n",
    "_ = backtesting_forecaster(\n",
    "        forecaster = forecaster,\n",
    "        y          = data,\n",
    "        cv         = cv,\n",
    "        metric     = 'mean_absolute_error'\n",
    "\n",
    "    )\n",
    "elapsed_time = pd.Timestamp.now() - start_time\n",
    "\n",
    "print(f\"Backtesting time using GPU: {elapsed_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "357f471b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time using CPU: 0 days 00:01:16.865061\n",
      "Prediction time using CPU: 0 days 00:00:00.168804\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Backtesting\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[32m     24\u001b[39m start_time = pd.Timestamp.now()\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m _ = \u001b[43mbacktesting_forecaster\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforecaster\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mforecaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m          \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m         \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmean_absolute_error\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     30\u001b[39m \n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m elapsed_time = pd.Timestamp.now() - start_time\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBacktesting time using CPU: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py11_2\\Lib\\site-packages\\skforecast\\model_selection\\_validation.py:583\u001b[39m, in \u001b[36mbacktesting_forecaster\u001b[39m\u001b[34m(forecaster, y, cv, metric, exog, interval, interval_method, n_boot, use_in_sample_residuals, use_binned_residuals, random_state, return_predictors, n_jobs, verbose, show_progress)\u001b[39m\n\u001b[32m    561\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    562\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`forecaster` must be of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mforecaters_allowed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, for all other types of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    563\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m forecasters use the functions available in the other `model_selection` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    564\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodules.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    565\u001b[39m     )\n\u001b[32m    567\u001b[39m check_backtesting_input(\n\u001b[32m    568\u001b[39m     forecaster              = forecaster,\n\u001b[32m    569\u001b[39m     cv                      = cv,\n\u001b[32m   (...)\u001b[39m\u001b[32m    580\u001b[39m     show_progress           = show_progress\n\u001b[32m    581\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m583\u001b[39m metric_values, backtest_predictions = \u001b[43m_backtesting_forecaster\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    584\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforecaster\u001b[49m\u001b[43m              \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mforecaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m                       \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m                      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m                  \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m                    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterval\u001b[49m\u001b[43m                \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minterval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterval_method\u001b[49m\u001b[43m         \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minterval_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_boot\u001b[49m\u001b[43m                  \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_boot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_in_sample_residuals\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_in_sample_residuals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_binned_residuals\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_binned_residuals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m            \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_predictors\u001b[49m\u001b[43m       \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_predictors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m                  \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m                 \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m           \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\n\u001b[32m    599\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m metric_values, backtest_predictions\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py11_2\\Lib\\site-packages\\skforecast\\model_selection\\_validation.py:224\u001b[39m, in \u001b[36m_backtesting_forecaster\u001b[39m\u001b[34m(forecaster, y, metric, cv, exog, interval, interval_method, n_boot, use_in_sample_residuals, use_binned_residuals, random_state, return_predictors, n_jobs, verbose, show_progress)\u001b[39m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m initial_train_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# First model training, this is done to allow parallelization when `refit`\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# is `False`. The initial Forecaster fit is outside the auxiliary function.\u001b[39;00m\n\u001b[32m    223\u001b[39m     exog_train = exog.iloc[:initial_train_size, ] \u001b[38;5;28;01mif\u001b[39;00m exog \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[43mforecaster\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m                         \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43minitial_train_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m                      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstore_in_sample_residuals\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_in_sample_residuals\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m     \u001b[38;5;66;03m# This is done to allow parallelization when `refit` is `False`. The initial \u001b[39;00m\n\u001b[32m    230\u001b[39m     \u001b[38;5;66;03m# Forecaster fit is outside the auxiliary function.\u001b[39;00m\n\u001b[32m    231\u001b[39m     folds[\u001b[32m0\u001b[39m][\u001b[32m4\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py11_2\\Lib\\site-packages\\skforecast\\recursive\\_forecaster_recursive.py:986\u001b[39m, in \u001b[36mForecasterRecursive.fit\u001b[39m\u001b[34m(self, y, exog, store_last_window, store_in_sample_residuals, random_state)\u001b[39m\n\u001b[32m    979\u001b[39m     \u001b[38;5;28mself\u001b[39m.regressor.fit(\n\u001b[32m    980\u001b[39m         X             = X_train,\n\u001b[32m    981\u001b[39m         y             = y_train,\n\u001b[32m    982\u001b[39m         sample_weight = sample_weight,\n\u001b[32m    983\u001b[39m         **\u001b[38;5;28mself\u001b[39m.fit_kwargs\n\u001b[32m    984\u001b[39m     )\n\u001b[32m    985\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m986\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mregressor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[38;5;28mself\u001b[39m.X_train_window_features_names_out_ = X_train_window_features_names_out_\n\u001b[32m    989\u001b[39m \u001b[38;5;28mself\u001b[39m.X_train_features_names_out_ = X_train_features_names_out_\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py11_2\\Lib\\site-packages\\xgboost\\core.py:726\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    724\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    725\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m726\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py11_2\\Lib\\site-packages\\xgboost\\sklearn.py:1108\u001b[39m, in \u001b[36mXGBModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1105\u001b[39m     obj = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1107\u001b[39m model, metric, params = \u001b[38;5;28mself\u001b[39m._configure_fit(xgb_model, params)\n\u001b[32m-> \u001b[39m\u001b[32m1108\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1120\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1122\u001b[39m \u001b[38;5;28mself\u001b[39m._set_evaluation_result(evals_result)\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py11_2\\Lib\\site-packages\\xgboost\\core.py:726\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    724\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    725\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m726\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py11_2\\Lib\\site-packages\\xgboost\\training.py:181\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    183\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast_py11_2\\Lib\\site-packages\\xgboost\\core.py:2101\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2097\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2099\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2100\u001b[39m     _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2101\u001b[39m         \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2102\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\n\u001b[32m   2103\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2104\u001b[39m     )\n\u001b[32m   2105\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2106\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Create and train forecaster with a XGBRegressor using CPU\n",
    "# ==============================================================================\n",
    "forecaster = ForecasterRecursive(\n",
    "                 regressor = XGBRegressor(n_estimators=1000),\n",
    "                 lags      = 50\n",
    "             )\n",
    "\n",
    "start_time = pd.Timestamp.now()\n",
    "forecaster.fit(y=data)\n",
    "elapsed_time = pd.Timestamp.now() - start_time\n",
    "\n",
    "print(f\"Training time using CPU: {elapsed_time}\")\n",
    "\n",
    "# Predict\n",
    "# ==============================================================================\n",
    "start_time = pd.Timestamp.now()\n",
    "forecaster.predict(steps=100)\n",
    "elapsed_time = pd.Timestamp.now() - start_time\n",
    "\n",
    "print(f\"Prediction time using CPU: {elapsed_time}\")\n",
    "\n",
    "# Backtesting\n",
    "# ==============================================================================\n",
    "start_time = pd.Timestamp.now()\n",
    "_ = backtesting_forecaster(\n",
    "        forecaster = forecaster,\n",
    "        y          = data,\n",
    "        cv         = cv,\n",
    "        metric     = 'mean_absolute_error'\n",
    "\n",
    "    )\n",
    "elapsed_time = pd.Timestamp.now() - start_time\n",
    "\n",
    "print(f\"Backtesting time using CPU: {elapsed_time}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a27b9011",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e6345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "# ==============================================================================\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\",\n",
    "    category=FutureWarning,\n",
    "    module=\"sklearn.utils.deprecation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600ac583",
   "metadata": {},
   "source": [
    "When using **Google colab**, run the following in a notebook cell to ensure LightGBM can utilize the NVIDIA GPU when executing in google colab.\n",
    "\n",
    "```bash\n",
    "!mkdir -p /etc/OpenCL/vendors && echo \"libnvidia-opencl.so.1\" > /etc/OpenCL/vendors/nvidia.icd\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da945f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time using GPU: 0 days 00:00:33.730939\n",
      "Prediction time using GPU: 0 days 00:00:00.072005\n"
     ]
    }
   ],
   "source": [
    "# Create and train forecaster with a LGBMRegressor using GPU\n",
    "# ==============================================================================\n",
    "forecaster = ForecasterRecursive(\n",
    "                 regressor = LGBMRegressor(n_estimators=1000, device='gpu', verbose=-1),\n",
    "                 lags      = 50\n",
    "             )\n",
    "\n",
    "start_time = pd.Timestamp.now()\n",
    "forecaster.fit(y=data)\n",
    "elapsed_time = pd.Timestamp.now() - start_time\n",
    "\n",
    "print(f\"Training time using GPU: {elapsed_time}\")\n",
    "\n",
    "# Predict\n",
    "# ==============================================================================\n",
    "start_time = pd.Timestamp.now()\n",
    "forecaster.predict(steps=100)\n",
    "elapsed_time = pd.Timestamp.now() - start_time\n",
    "\n",
    "print(f\"Prediction time using GPU: {elapsed_time}\")\n",
    "\n",
    "# Backtesting\n",
    "# ==============================================================================\n",
    "start_time = pd.Timestamp.now()\n",
    "_ = backtesting_forecaster(\n",
    "        forecaster = forecaster,\n",
    "        y          = data,\n",
    "        cv         = cv,\n",
    "        metric     = 'mean_absolute_error'\n",
    "\n",
    "    )\n",
    "elapsed_time = pd.Timestamp.now() - start_time\n",
    "\n",
    "print(f\"Backtesting time using GPU: {elapsed_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdbf58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time using CPU: 0 days 00:00:38.861726\n",
      "Prediction time using GPU: 0 days 00:00:00.090388\n"
     ]
    }
   ],
   "source": [
    "# Create and train forecaster with a LGBMRegressor using CPU\n",
    "# ==============================================================================\n",
    "forecaster = ForecasterRecursive(\n",
    "                 regressor = LGBMRegressor(n_estimators=1000, device='cpu', verbose=-1),\n",
    "                 lags      = 50\n",
    "             )\n",
    "\n",
    "start_time = pd.Timestamp.now()\n",
    "forecaster.fit(y=data)\n",
    "elapsed_time = pd.Timestamp.now() - start_time\n",
    "\n",
    "print(f\"Training time using CPU: {elapsed_time}\")\n",
    "\n",
    "# Predict\n",
    "# ==============================================================================\n",
    "start_time = pd.Timestamp.now()\n",
    "forecaster.predict(steps=100)\n",
    "elapsed_time = pd.Timestamp.now() - start_time\n",
    "\n",
    "print(f\"Prediction time using CPU: {elapsed_time}\")\n",
    "\n",
    "# Backtesting\n",
    "# ==============================================================================\n",
    "start_time = pd.Timestamp.now()\n",
    "_ = backtesting_forecaster(\n",
    "        forecaster = forecaster,\n",
    "        y          = data,\n",
    "        cv         = cv,\n",
    "        metric     = 'mean_absolute_error'\n",
    "\n",
    "    )\n",
    "elapsed_time = pd.Timestamp.now() - start_time\n",
    "\n",
    "print(f\"Backtesting time using CPU: {elapsed_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae97578",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff7c2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time using GPU: 0 days 00:00:27.542290\n",
      "Prediction time using GPU: 0 days 00:00:00.136249\n"
     ]
    }
   ],
   "source": [
    "# Create and train forecaster with a CatBoostRegressor using GPU\n",
    "# ==============================================================================\n",
    "forecaster = ForecasterRecursive(\n",
    "                 regressor = CatBoostRegressor(n_estimators=1000, task_type='GPU', silent=True, allow_writing_files=False),\n",
    "                 lags      = 50\n",
    "             )\n",
    "\n",
    "start_time = pd.Timestamp.now()\n",
    "forecaster.fit(y=data)\n",
    "elapsed_time = pd.Timestamp.now() - start_time\n",
    "\n",
    "print(f\"Training time using GPU: {elapsed_time}\")\n",
    "\n",
    "# Predict\n",
    "# ==============================================================================\n",
    "start_time = pd.Timestamp.now()\n",
    "forecaster.predict(steps=100)\n",
    "elapsed_time = pd.Timestamp.now() - start_time\n",
    "\n",
    "print(f\"Prediction time using GPU: {elapsed_time}\")\n",
    "\n",
    "# Backtesting\n",
    "# ==============================================================================\n",
    "start_time = pd.Timestamp.now()\n",
    "_ = backtesting_forecaster(\n",
    "        forecaster = forecaster,\n",
    "        y          = data,\n",
    "        cv         = cv,\n",
    "        metric     = 'mean_absolute_error'\n",
    "\n",
    "    )\n",
    "elapsed_time = pd.Timestamp.now() - start_time\n",
    "\n",
    "print(f\"Backtesting time using GPU: {elapsed_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ab6127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time using CPU: 0 days 00:01:20.693267\n",
      "Prediction time using GPU: 0 days 00:00:00.094369\n"
     ]
    }
   ],
   "source": [
    "# Create and train forecaster with a CatBoostRegressor using CPU\n",
    "# ==============================================================================\n",
    "forecaster = ForecasterRecursive(\n",
    "                 regressor = CatBoostRegressor(n_estimators=1000, task_type='CPU', silent=True, allow_writing_files=False),\n",
    "                 lags      = 50\n",
    "             )\n",
    "\n",
    "start_time = pd.Timestamp.now()\n",
    "forecaster.fit(y=data)\n",
    "elapsed_time = pd.Timestamp.now() - start_time\n",
    "\n",
    "print(f\"Training time using CPU: {elapsed_time}\")\n",
    "\n",
    "# Predict\n",
    "# ==============================================================================\n",
    "start_time = pd.Timestamp.now()\n",
    "forecaster.predict(steps=100)\n",
    "elapsed_time = pd.Timestamp.now() - start_time\n",
    "\n",
    "print(f\"Prediction time using CPU: {elapsed_time}\")\n",
    "\n",
    "# Backtesting\n",
    "# ==============================================================================\n",
    "start_time = pd.Timestamp.now()\n",
    "_ = backtesting_forecaster(\n",
    "        forecaster = forecaster,\n",
    "        y          = data,\n",
    "        cv         = cv,\n",
    "        metric     = 'mean_absolute_error'\n",
    "\n",
    "    )\n",
    "elapsed_time = pd.Timestamp.now() - start_time\n",
    "\n",
    "print(f\"Backtesting time using CPU: {elapsed_time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skforecast_py11_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
